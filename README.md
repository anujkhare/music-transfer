# Transfer learning in music data!

## Problem statement
### What?

A general "language-model" like pre-trained model for music modeling that can be used for easily
performing downstream tasks in music recognition with a shallow model.


### Why?

Transfer learning has been very successful in the image domain. Any classification / detection task
based on real world images benefits greatly from pre-trained models.

These are the tasks at hand:

### Transfer learning makes sense, but why do this for music data? Does it even make sense?
#### Tasks / problems that deal with music

#### Is deep learning even needed in music? Where?

#### Aren't most music tasks sequential? Would this even help there?
Ideally, any place that requires understand or memorizing a long-term structure for the music would benefit from this.

#### Is there any related work or precedent for this?

### Okay, this does make sense. Why now?
Two recent successes motivate this thought:
1. OpenAI GPT2: unsupervised training on a very large text corpus producing unprecedented language modeling and understanding
    capabilities. Similar results were demonstrated by ULMfit, BeRT, and ELMo.
2. Music transformer: An attention based model that learned to model music better than ever before?

### Is this even doable? What are the major challenges?

1. Data
2. Model
3. Ability to model music successfully
4. Ability to use the features in the downstream tasks
5. Raw compute and time x|


### How is it different from the work done in X?

| --- | --- | --- | --- |
| Previous work | Their contribution | Difference | Challenges |
| [Music transformer]() | bar contribution | haha | ho |
| [OpenAI GPT2]() | bar contribution | haha | ho |
| [Transfer learning for music]() | bar contribution | haha | ho |
| [foo bar]() | bar contribution | haha | ho |


## Details 
### Baseline model?
### Data collection
### Target tasks
#### Baselines for each?

### What are the success criteria?
